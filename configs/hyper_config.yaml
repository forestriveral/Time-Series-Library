# Experiments configuartion finetuning using batch runner script

# forecasting task
seq_len: [128, 256, 384]
label_len: 96
pred_len: 96

# model define
top_k: 5
num_kernels: 6

d_model: [512]
n_heads: [4, 8, 16]
e_layers: [2, 3, 4]
d_layers: [1, 2, 3]
d_ff: [2048]

moving_avg: 25
factor: 1
dropout: 0.2

# optimization
itr: 3
train_epochs: 10
batch_size: 32
patience: 3
learning_rate: 0.0001
